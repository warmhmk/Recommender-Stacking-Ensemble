# Recommender System Using Stacking Ensemble with Random Forest Regression

This project demonstrates the implementation of a sophisticated recommender system using a stacking ensemble method. The approach leverages multiple collaborative filtering models and combines their predictions using a Random Forest regressor. Stacking as a method is often underutilized in traditional recommender systems, but this project showcases its potential to enhance prediction accuracy by using predictions from different models as inputs for further training.

## Overview

The key challenge addressed in this project is the limited data available for training the final model. While standard train-test splits might lead to overfitting and poor generalization, this project utilizes a stacking approach combined with cross-validation to mitigate this issue. By training multiple base models on different data splits and stacking their predictions, the Random Forest regressor can be trained on a more comprehensive dataset, leading to better generalization and improved prediction accuracy.

## Stacking Methodology

### Cross-Validation and Stacking

1. **Cross-Validation**: The dataset contains about 700,000 rows of training data. Instead of a simple train-test split, the project uses 5-fold cross-validation to generate multiple train-test splits. Each fold has approximately 500,000 rows for training and 100,000 rows for testing.

2. **Base Model Training**: During each fold, multiple collaborative filtering models (such as SVD and NMF) are trained on the training subset and then used to predict ratings for the test subset within that fold.

3. **Prediction Collection**: Unlike typical cross-validation where the goal is to evaluate performance, here the focus is on collecting predictions for each test fold. After running through all folds, the project has predictions from both SVD and NMF for all the rows in the training data (totaling 700,000 rows).

4. **Combining Predictions**: The collected predictions from all folds are then used to form a comprehensive dataset. This dataset includes not just the predictions from SVD and NMF, but also additional features such as user average ratings, total votes, and helpful votes.

5. **Training the Meta-Model**: This enriched dataset, now containing 700,000 rows with predictions and additional features, is used to train the Random Forest regressor. The Random Forest model acts as a meta-model, learning how to best combine the inputs from the base models to make the final predictions.

### Final Prediction

When making final predictions on the test data:

- The Random Forest model uses the stacked predictions (e.g., from SVD and NMF) and the additional features (e.g., user average rating, total votes, helpful votes) as inputs.
- The final predictions are then generated by the Random Forest model and saved to a CSV file for submission.

### Advantages of the Stacking Approach

- **Better Generalization**: By collecting predictions across multiple cross-validation folds and training the Random Forest model on this comprehensive set, the model generalizes better to unseen data.
- **Reduction of Overfitting**: The larger and more diverse training set for the Random Forest regressor helps mitigate the risk of overfitting, which is a common issue when using limited training data.
- **Flexibility**: The stacking framework allows for easy adjustments, such as incorporating additional base models or more features, to further improve prediction accuracy.

## Key Features

- **Ensemble Learning**: Combines predictions from multiple collaborative filtering models to improve accuracy.
- **Cross-Validation Stacking**: Utilizes cross-validation to generate diverse training data for the Random Forest model, enhancing generalization.
- **GPU Acceleration**: Leverages RAPIDS for faster model training on GPU, significantly reducing computation time.
- **Comprehensive Feature Set**: Incorporates additional features such as user rating patterns, total votes, and helpful votes to boost prediction accuracy.

## Running Instructions

### Prerequisites

- **GPU Requirement**: The code requires a T4 GPU to run efficiently due to the use of GPU-accelerated libraries like RAPIDS.
- **High RAM**: High memory is recommended for handling large datasets during training.

### Steps to Run the Code

1. **Mount Google Drive**: Access data and code by mounting Google Drive in Google Colab.
   ```
   from google.colab import drive
   drive.mount('/content/drive')
   ```

2. **Install Required Libraries**: Install necessary libraries, including RAPIDS for GPU-accelerated random forest training.
   ```
   !pip install implicit
   !pip install scikit-surprise
   !git clone https://github.com/rapidsai/rapidsai-csp-utils.git
   !python rapidsai-csp-utils/colab/pip-install.py
   ```

3. **Import Libraries**: Import all required libraries for data processing, model training, and evaluation.
   ```
   import implicit
   from sklearn.preprocessing import MinMaxScaler
   from sklearn.metrics import mean_squared_error
   from sklearn.model_selection import KFold
   from sklearn.ensemble import RandomForestRegressor
   from surprise import SVD, NMF, Dataset, Reader
   import torch
   import pandas as pd
   import numpy as np
   from tqdm import tqdm
   ```

4. **Load and Explore Data**: Load the training and test datasets, and perform initial exploration to understand the data structure.
   ```
   train_df = pd.read_csv('/train.csv')
   test_df = pd.read_csv('/test.csv')
   print(train_df.shape, test_df.shape)
   ```

5. **Cross-Validation and Stacking**: Run the cross-validation process to generate predictions from base models and stack them.
   ```
   kf = KFold(n_splits=5, shuffle=True, random_state=44)
   # Train base models and collect predictions across all folds
   ```

6. **Train Random Forest**: Use the collected predictions and additional features to train the Random Forest model.
   ```
   rf = RandomForestRegressor(n_estimators=150, max_depth=10, random_state=44, n_streams=1)
   rf.fit(X_train, y_train)
   ```

7. **Make Final Predictions**: Generate the final predictions for the test data using the trained Random Forest model.
   ```
   y_pred = rf.predict(X_val)
   res = pd.DataFrame({'ID': range(len(y_pred)), 'rating': y_pred})
   res.to_csv('submission.csv', index=False)
   ```

## Conclusion

This project highlights the power of stacking ensemble methods, particularly in scenarios with limited training data. By leveraging
